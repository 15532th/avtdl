import asyncio
import logging
import os
import re
from pathlib import Path
from typing import List, Mapping, Optional, Sequence

from pydantic import AnyUrl, Field, NonNegativeFloat, RootModel, ValidationError, field_validator

from avtdl.core import utils
from avtdl.core.actions import QueueAction, QueueActionConfig, QueueActionEntity
from avtdl.core.cache import FileCache, is_url
from avtdl.core.download import RemoteFileInfo, download_file, has_same_content, remove_files
from avtdl.core.interfaces import Record, RuntimeContext
from avtdl.core.plugins import Plugins
from avtdl.core.request import HttpClient
from avtdl.core.utils import Fmt, check_dir, sanitize_filename, sha1


@Plugins.register('download', Plugins.kind.ACTOR_CONFIG)
class FileDownloadConfig(QueueActionConfig):
    max_concurrent_downloads: int = Field(default=1, ge=1)
    """limit for simultaneously active download tasks among all entities. Note that each entity will still process records sequentially regardless of this setting"""
    partial_file_suffix: str = '.part'
    """appended to a name of the file that is not yet completely downloaded"""


@Plugins.register('download', Plugins.kind.ACTOR_ENTITY)
class FileDownloadEntity(QueueActionEntity):
    url_field: str
    """field in the incoming record containing url of file to be downloaded"""
    path: Path
    """directory where downloaded file should be created. Supports templating with {...}"""
    filename: Optional[str] = None
    """name downloaded file should be stored under. If not provided will be inferred from HTTP headers or download url. Supports templating with {...} (additionally, "{source_name}" placeholder will be replaced with the inferred value)"""
    extension: Optional[str] = None
    """normally file extension will be inferred from HTTP headers. This option allows to overwrite it"""
    overwrite: bool = False
    """whether file should be overwritten in if it already exists. If set to false will cause suffix with a number be added to the newly downloaded file name"""
    rename_suffix: str = ' [{i}]'
    """when overwriting is disabled, this suffix is attached to the base filename with the "{i}" part replaced with a number. Must contain "{i}" exactly once"""

    @field_validator('extension')
    @classmethod
    def ensure_dot(cls, value: Optional[str]) -> Optional[str]:
        if value is None:
            return None
        if value.startswith('.'):
            return value
        return '.' + value

    @field_validator('rename_suffix')
    @classmethod
    def check_suffix(cls, value: str) -> str:
        found = len(re.findall(r'{i}', value))
        if found != 1:
            raise ValueError('rename_suffix must contain exactly one occurrence of "{i}", got ' + str(found))
        value = sanitize_filename(value)
        return value


@Plugins.register('download', Plugins.kind.ACTOR)
class FileDownload(QueueAction):
    """
    Download a file

    Take an url from a field of a processed record with a name specified in `url_field`
    and download it as a file to specified location.
    The field must be present in the record and  must contain a valid url with a scheme
    (such as "https") or a list of such urls.

    Primarily designed for downloading images attached to a post, or thumbnails.
    Does not support resuming interrupted downloads or detecting that this exact file is
    already stored at target location without downloading it again.

    File extension and name are inferred from HTTP headers and path part of the url,
    unless provided explicitly with `extension` and `filename` parameters.
    Since final file name is determined as a part of the download process, the file
    is initially stored under a temporary name (currently an SHA1 of the url) in the
    download directory, and then renamed to target filename.

    If a file with given name already exists, depending on an `overwrite` setting a new file will either
    overwrite it or get stored under different name, generated by combining
    the base name with a number added as part of `rename_suffix`.
    If, however, an exact copy of the new file is found among the files in target directory
    sharing the base name, the new file will be deleted, giving preference to the existing copy.
    """

    def __init__(self, conf: FileDownloadConfig, entities: Sequence[FileDownloadEntity], ctx: RuntimeContext):
        super().__init__(conf, entities, ctx)
        self.conf: FileDownloadConfig
        self.entities: Mapping[str, FileDownloadEntity]
        self.concurrency_limit = asyncio.BoundedSemaphore(value=conf.max_concurrent_downloads)

    def _get_urls_list(self, entity: FileDownloadEntity, record: Record) -> Optional[List[str]]:
        field = getattr(record, entity.url_field, None)
        if field is None:
            msg = f'received a record that does not contain "{entity.url_field}" field. The record: {record!r}'
            self.logger.debug(msg)
            return None
        if isinstance(field, str):
            field = [field]
        try:
            urls = [str(url) for url in UrlList(field)]
            return urls
        except ValidationError:
            self.logger.debug(
                f'received record with the "{entity.url_field}" field that was not recognised as a valid url or a sequence of urls. Raw record: {record!r}')
            return None

    async def handle_single_record(self, logger: logging.Logger, client: HttpClient,
                                   entity: FileDownloadEntity, record: Record) -> None:
        urls = self._get_urls_list(entity, record)
        if urls is None:
            self.logger.debug(f'found no values in field "{entity.url_field}", skipping record')
            return
        for url in urls:
            self.logger.debug(f'processing url {url}')
            await self.handle_download(logger, client, entity, record, url)

    async def handle_download(self, logger, client: HttpClient, entity: FileDownloadEntity, record: Record, url: str):
        """
        Handle download-related stuff: generating filenames, moving files, error reporting

        - generate tempfile name from url hash
        - if it exists abort
        - perform the download into a temp file
        - generate resulting file name
        - if exists either rename or replace depending on settings
        """
        path = Fmt.format_path(entity.path, record, tz=entity.timezone)
        ok = check_dir(path)
        if not ok:
            logger.warning(f'check "{path}" is a valid and writeable directory')
            return

        temp_file = path / Path(sha1(url)).with_suffix(self.conf.partial_file_suffix)
        if temp_file.exists():
            logger.warning(
                f'aborting download of "{url}": temporary file "{temp_file}" already exists, meaning download is already in progress or download process has been interrupted abruptly')
            return

        logger.debug(f'downloading "{url}" to "{temp_file}"')
        info = await self.download(logger, client, url, temp_file)
        if info is None:
            return None

        if entity.filename is not None:
            extra = {'source_name': info.source_name}
            filename = Fmt.format(entity.filename, record, tz=entity.timezone, extra=extra)
        else:
            filename = info.source_name
        filename = sanitize_filename(filename)
        path = path.joinpath(filename)
        if entity.extension is not None:
            path = path.with_suffix(entity.extension)
        else:
            path = path.with_suffix(info.extension)

        try:
            path.exists()
        except OSError as e:
            logger.warning(f'failed to process record: {e}')
            return
        if path.exists() and not entity.overwrite:
            for p in path.parent.iterdir():
                if not p.stem.startswith(path.stem):
                    continue
                if has_same_content(temp_file, p):
                    self.logger.info(f'file "{temp_file}" is already stored as "{p}", deleting')
                    remove_files([temp_file])
                    return
            new_path = Path(path)  # making a copy
            i = 0
            while new_path.exists():
                i += 1
                suffix = entity.rename_suffix.replace('{i}', str(i))
                new_name = path.stem + suffix
                new_path = new_path.with_stem(new_name)
            path = new_path
        move_file(temp_file, path, logger)

    async def download(self, logger: logging.Logger, client: HttpClient,
                       url: str, output_file: Path) -> Optional[RemoteFileInfo]:
        """Perform the actual download"""
        return await download(self.concurrency_limit, logger, client, url, output_file)


async def download(semaphore: asyncio.BoundedSemaphore, logger: logging.Logger, client: HttpClient,
                   url: str, output_file: Path) -> Optional[RemoteFileInfo]:
    try:
        async with semaphore:
            logger.debug(
                f'acquired semaphore({semaphore._value}), downloading "{url}" to "{output_file}"')
            info = await download_file(url, output_file, client.session)
    except Exception as e:
        logger.exception(f'unexpected error when downloading "{url}" to "{output_file}": {e}')
        return None
    logger.debug(
        f'finished downloading "{url}" to "{output_file}", semaphore({semaphore._value}) released')
    return info


class UrlList(RootModel):
    root: Sequence[AnyUrl]

    def __iter__(self):
        return iter(self.root)

    def __getitem__(self, item):
        return self.root[item]


def move_file(source: Path, target: Path, logger: logging.Logger) -> bool:
    try:
        logger.debug(f'moving "{source}" to "{target}"')
        os.replace(source, target)
        return True
    except Exception as e:
        message = f'failed to move file "{source}" to desired location "{target}": {e}'
        logger.warning(message)
        return False


@Plugins.register('cache', Plugins.kind.ACTOR_CONFIG)
class FileCacheConfig(FileDownloadConfig):
    cache_directory: Path = Field(default='cache/downloads/', validate_default=True)
    """base directory to store cached resources"""

    @field_validator('cache_directory')
    @classmethod
    def check_dir(cls, path: Path):
        ok = utils.check_dir(path)
        if ok:
            return path
        else:
            raise ValueError(f'check path "{path}" exists and is a writeable directory')


@Plugins.register('cache', Plugins.kind.ACTOR_ENTITY)
class FileCacheEntity(QueueActionEntity):
    url_fields: List[str] = ['attachments', 'thumbnail_url', 'avatar_url']
    """names of fields in the incoming record containing urls of files to be downloaded"""
    replace_after: Optional[NonNegativeFloat] = None
    """how old existing file should be to get redownloaded, in hours"""


@Plugins.register('cache', Plugins.kind.ACTOR)
class FileCacheAction(QueueAction):
    """
    Cache url locally

    For every incoming record, go through fields specified in "url_fields" setting
    and download files the urls are pointing to. Downloaded files are then used
    to present record in the web interface.
    """

    def __init__(self, conf: FileCacheConfig, entities: Sequence[FileCacheEntity], ctx: RuntimeContext):
        super().__init__(conf, entities, ctx)
        self.conf: FileCacheConfig
        self.entities: Mapping[str, FileCacheEntity]
        self.concurrency_limit = asyncio.BoundedSemaphore(value=conf.max_concurrent_downloads)
        self.cache = FileCache(self.conf.cache_directory, self.conf.partial_file_suffix)

    async def handle_single_record(self, logger: logging.Logger, client: HttpClient,
                                   entity: FileCacheEntity, record: Record) -> None:
        for field in entity.url_fields:
            await self._handle_field(logger, client, record, field, entity.replace_after)

    async def _handle_field(self, logger: logging.Logger, client: HttpClient,
                            record: Record, field_name: str, replace_after: Optional[float]):
        field = getattr(record, field_name, None)
        if field is None:
            logger.debug(f'no field "{field_name}" in record {record!r}, skipping')
        elif isinstance(field, str):
            await self._cache_urls(logger, client, record, [field], replace_after)
        elif isinstance(field, list):
            await self._cache_urls(logger, client, record, field, replace_after)
        else:
            msg = f'field "{field_name}" of record {record!r} does not seem to hold any links. Raw field value: {field}'
            logger.debug(msg)

    async def _cache_urls(self, logger: logging.Logger, client: HttpClient,
                          record: Record, maybe_urls: List[str], replace_after: Optional[float]) -> None:
        for url in maybe_urls:
            if is_url(url):
                async with self.concurrency_limit:
                    await self.cache.store(logger, client, record, url, replace_after)
            else:
                msg = f'"{url}" does not seem to be a valid url, skipping. Record: {record!r}'
                logger.debug(msg)
