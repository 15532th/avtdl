import asyncio
import logging
import os
import re
from pathlib import Path
from typing import Dict, List, Mapping, Optional, Sequence

from pydantic import AnyHttpUrl, AnyUrl, Field, RootModel, ValidationError, field_validator, model_validator

from avtdl.core.actions import QueueAction, QueueActionConfig, QueueActionEntity
from avtdl.core.download import RemoteFileInfo, download_file, has_same_content, remove_files
from avtdl.core.interfaces import Record, RuntimeContext
from avtdl.core.plugins import Plugins
from avtdl.core.request import HttpClient
from avtdl.core.utils import Fmt, check_dir, sanitize_filename, sha1


@Plugins.register('download', Plugins.kind.ACTOR_CONFIG)
class FileDownloadConfig(QueueActionConfig):
    max_concurrent_downloads: int = Field(default=1, ge=1)
    """limit for simultaneously active download tasks among all entities. Note that each entity will still process records sequentially regardless of this setting"""
    partial_file_suffix: str = '.part'
    """appended to a name of the file that is not yet completely downloaded"""


@Plugins.register('download', Plugins.kind.ACTOR_ENTITY)
class FileDownloadEntity(QueueActionEntity):
    url_field: str
    """field in the incoming record containing url of file to be downloaded"""
    path: Path
    """directory where downloaded file should be created. Supports templating with {...}"""
    filename: Optional[str] = None
    """name downloaded file should be stored under. If not provided will be inferred from HTTP headers or download url. Supports templating with {...} (additionally, "{source_name}" placeholder will be replaced with the inferred value)"""
    extension: Optional[str] = None
    """normally file extension will be inferred from HTTP headers. This option allows to overwrite it"""
    overwrite: bool = False
    """whether file should be overwritten in if it already exists. If set to false will cause suffix with a number be added to the newly downloaded file name"""
    rename_suffix: str = ' [{i}]'
    """when overwriting is disabled, this suffix is attached to the base filename with the "{i}" part replaced with a number. Must contain "{i}" exactly once"""

    @field_validator('extension')
    @classmethod
    def ensure_dot(cls, value: Optional[str]) -> Optional[str]:
        if value is None:
            return None
        if value.startswith('.'):
            return value
        return '.' + value

    @field_validator('rename_suffix')
    @classmethod
    def check_suffix(cls, value: str) -> str:
        found = len(re.findall(r'{i}', value))
        if found != 1:
            raise ValueError('rename_suffix must contain exactly one occurrence of "{i}", got ' + str(found))
        value = sanitize_filename(value)
        return value


@Plugins.register('download', Plugins.kind.ACTOR)
class FileDownload(QueueAction):
    """
    Download a file

    Take an url from a field of a processed record with a name specified in `url_field`
    and download it as a file to specified location.
    The field must be present in the record and  must contain a valid url with a scheme
    (such as "https") or a list of such urls.

    Primarily designed for downloading images attached to a post, or thumbnails.
    Does not support resuming interrupted downloads or detecting that this exact file is
    already stored at target location without downloading it again.

    File extension and name are inferred from HTTP headers and path part of the url,
    unless provided explicitly with `extension` and `filename` parameters.
    Since final file name is determined as a part of the download process, the file
    is initially stored under a temporary name (currently an SHA1 of the url) in the
    download directory, and then renamed to target filename.

    If a file with given name already exists, depending on an `overwrite` setting a new file will either
    overwrite it or get stored under different name, generated by combining
    the base name with a number added as part of `rename_suffix`.
    If, however, an exact copy of the new file is found among the files in target directory
    sharing the base name, the new file will be deleted, giving preference to the existing copy.
    """

    def __init__(self, conf: FileDownloadConfig, entities: Sequence[FileDownloadEntity], ctx: RuntimeContext):
        super().__init__(conf, entities, ctx)
        self.conf: FileDownloadConfig
        self.entities: Mapping[str, FileDownloadEntity]
        self.concurrency_limit = asyncio.BoundedSemaphore(value=conf.max_concurrent_downloads)

    def _get_urls_list(self, entity: FileDownloadEntity, record: Record) -> Optional[List[str]]:
        field = getattr(record, entity.url_field, None)
        if field is None:
            msg = f'received a record that does not contain "{entity.url_field}" field. The record: {record!r}'
            self.logger.debug(msg)
            return None
        if isinstance(field, str):
            field = [field]
        try:
            urls = [str(url) for url in UrlList(field)]
            return urls
        except ValidationError:
            self.logger.debug(
                f'received record with the "{entity.url_field}" field that was not recognised as a valid url or a sequence of urls. Raw record: {record!r}')
            return None

    async def handle_single_record(self, logger: logging.Logger, client: HttpClient,
                                   entity: FileDownloadEntity, record: Record) -> None:
        urls = self._get_urls_list(entity, record)
        if urls is None:
            self.logger.debug(f'found no values in field "{entity.url_field}", skipping record')
            return
        for url in urls:
            self.logger.debug(f'processing url {url}')
            await self.handle_download(logger, client, entity, record, url)

    async def handle_download(self, logger, client: HttpClient, entity: FileDownloadEntity, record: Record, url: str):
        """
        Handle download-related stuff: generating filenames, moving files, error reporting

        - generate tempfile name from url hash
        - if it exists abort
        - perform the download into a temp file
        - generate resulting file name
        - if exists either rename or replace depending on settings
        """
        path = Fmt.format_path(entity.path, record, tz=entity.timezone)
        ok = check_dir(path)
        if not ok:
            logger.warning(f'check "{path}" is a valid and writeable directory')
            return

        temp_file = path / Path(sha1(url)).with_suffix(self.conf.partial_file_suffix)
        if temp_file.exists():
            logger.warning(
                f'aborting download of "{url}": temporary file "{temp_file}" already exists, meaning download is already in progress or download process has been interrupted abruptly')
            return

        logger.debug(f'downloading "{url}" to "{temp_file}"')
        info = await self.download(logger, client, url, temp_file)
        if info is None:
            return None

        if entity.filename is not None:
            extra = {'source_name': info.source_name}
            filename = Fmt.format(entity.filename, record, tz=entity.timezone, extra=extra)
        else:
            filename = info.source_name
        filename = sanitize_filename(filename)
        path = path.joinpath(filename)
        if entity.extension is not None:
            path = path.with_suffix(entity.extension)
        else:
            path = path.with_suffix(info.extension)

        try:
            path.exists()
        except OSError as e:
            logger.warning(f'failed to process record: {e}')
            return
        if path.exists() and not entity.overwrite:
            for p in path.parent.iterdir():
                if not p.stem.startswith(path.stem):
                    continue
                if has_same_content(temp_file, p):
                    self.logger.info(f'file "{temp_file}" is already stored as "{p}", deleting')
                    remove_files([temp_file])
                    return
            new_path = Path(path)  # making a copy
            i = 0
            while new_path.exists():
                i += 1
                suffix = entity.rename_suffix.replace('{i}', str(i))
                new_name = path.stem + suffix
                new_path = new_path.with_stem(new_name)
            path = new_path
        move_file(temp_file, path, logger)

    async def download(self, logger: logging.Logger, client: HttpClient,
                       url: str, output_file: Path) -> Optional[RemoteFileInfo]:
        """Perform the actual download"""
        return await download(self.concurrency_limit, logger, client, url, output_file)


async def download(semaphore: asyncio.BoundedSemaphore, logger: logging.Logger, client: HttpClient,
                   url: str, output_file: Path) -> Optional[RemoteFileInfo]:
    try:
        async with semaphore:
            logger.debug(
                f'acquired semaphore({semaphore._value}), downloading "{url}" to "{output_file}"')
            info = await download_file(url, output_file, client.session)
    except Exception as e:
        logger.exception(f'unexpected error when downloading "{url}" to "{output_file}": {e}')
        return None
    logger.debug(
        f'finished downloading "{url}" to "{output_file}", semaphore({semaphore._value}) released')
    return info


class UrlList(RootModel):
    root: Sequence[AnyUrl]

    def __iter__(self):
        return iter(self.root)

    def __getitem__(self, item):
        return self.root[item]


def move_file(source: Path, target: Path, logger: logging.Logger) -> bool:
    try:
        logger.debug(f'moving "{source}" to "{target}"')
        os.replace(source, target)
        return True
    except Exception as e:
        message = f'failed to move file "{source}" to desired location "{target}": {e}'
        logger.warning(message)
        return False


def find_file(path: Path) -> List[Path]:
    """given path to a file without extension, return list of existing files with exact path and name and any extension"""
    if not path.parent.exists():
        return []
    try:
        return [p for p in path.parent.iterdir() if p.stem == path.stem]
    except OSError:
        return []


def is_url(maybe_url: str) -> bool:
    try:
        AnyHttpUrl(maybe_url)
        return True
    except ValidationError:
        return False


@Plugins.register('cache', Plugins.kind.ACTOR_CONFIG)
class FileCacheConfig(FileDownloadConfig):
    cache_directory: Path = Field(default='cache/downloads/', validate_default=True)
    """base directory to store cached resources. Shared across all entities"""


@Plugins.register('cache', Plugins.kind.ACTOR_ENTITY)
class FileCacheEntity(QueueActionEntity):
    url_fields: List[str] = ['attachments', 'thumbnail_url', 'avatar_url']
    """names of fields in the incoming record containing urls of files that has to be downloaded"""
    no_reuse: bool = False
    """force download even if file from the same url was already stored in cache"""
    resources_directory: Path = Field(default='cache/downloads/', validate_default=True)
    """base directory to look for externally provided resources"""
    resources_paths: Dict[str, str] = {}
    """paths to directories with externally provided resources, defined as mapping "field name": "path". Supports templating with {}"""
    resources_files: Dict[str, str] = {}
    """filename templates of the externally provided resources, defined as mapping "field name": "file name".
    Supports templating with {}. Additionally, "{i}" might be used as a placeholder for any number,
    allowing to specify multiple files with one template"""

    @model_validator(mode='after')
    def handle_paths(self) -> 'FileCacheEntity':
        resource_files_fields = set(self.resources_files.keys())
        resource_paths_fields = set(self.resources_paths.keys())
        if resource_files_fields != resource_paths_fields:
            raise ValueError(f'resources_files and resources_paths must define exactly the same record fields')
        return self


def find_free_suffix(path: Path, suffix_template: str) -> Path:
    """given path to possible existing file name, find a free name
    generated by appending suffix_template to original filename.
    The suffix_template parameter must contain "{i}" exactly once"""
    if suffix_template.count("{i}") != 1:
        raise ValueError("The suffix_template must contain '{i}' exactly once.")

    base_name = path.stem
    new_path = path
    i = 0

    while new_path.exists():
        i += 1
        suffix = suffix_template.format(i=i)
        new_path = path.with_name(f"{base_name}{suffix}").with_suffix(path.suffix)
    return new_path


@Plugins.register('cache', Plugins.kind.ACTOR)
class FileCache(QueueAction):
    """
    Cache url locally

    For every incoming record, go through fields specified in "url_fields" setting,
    download files the urls are pointing to, and overwrite the urls with
    local "file://" links to downloaded files before emitting record down the chain.
    """

    def __init__(self, conf: FileCacheConfig, entities: Sequence[FileCacheEntity], ctx: RuntimeContext):
        super().__init__(conf, entities, ctx)
        self.conf: FileCacheConfig
        self.entities: Mapping[str, FileCacheEntity]
        self.concurrency_limit = asyncio.BoundedSemaphore(value=conf.max_concurrent_downloads)

    async def handle_single_record(self, logger: logging.Logger, client: HttpClient,
                                   entity: FileCacheEntity, record: Record) -> None:
        for field in entity.url_fields:
            await self._handle_field(logger, client, entity, record, field)
        # record fields should have been updated with local files if possible
        self.on_record(entity, record)

    async def _handle_field(self, logger: logging.Logger, client: HttpClient,
                            entity: FileCacheEntity, record: Record, field_name: str):
        field = getattr(record, field_name, None)
        if field is None:
            logger.debug(f'no field "{field_name}" in record {record!r}, skipping')
        elif isinstance(field, str):
            [new_url] = await self._cache_urls(logger, client, entity, record, field_name, [field])
            setattr(record, field_name, new_url)
        elif isinstance(field, list):
            new_urls = await self._cache_urls(logger, client, entity, record, field_name, field)
            setattr(record, field_name, new_urls)
        else:
            logger.debug(
                f'field "{field_name}" of record {record!r} does not seem to hold any links. Raw field value: {field}')

    async def _cache_urls(self, logger: logging.Logger, client: HttpClient,
                          entity: FileCacheEntity, record: Record,
                          field_name: str, maybe_urls: List[str]) -> List[str]:
        """try storing urls locally, for each return uri of local path if succeeded or url on error"""
        updated_urls = []
        for url in maybe_urls:
            if not is_url(url):
                logger.debug(f'value {url} of field "{field_name}" of record {record!r} is not an url, skipping')
                updated_urls.append(url)
                continue
            paths = await self._ensure_stored(logger, client, entity, record, field_name, url)
            if not paths:
                updated_urls.append(url)
                continue
            elif len(paths) == 1:
                updated_urls.append(paths[0].absolute().as_uri())
            elif len(paths) > 1:
                logger.warning(
                    f'found multiple files suitable for url "{url}" in field "{field_name}" of record "{record!r}": {paths}')
                updated_urls.append(paths[0].absolute().as_uri())
        return updated_urls

    async def _ensure_stored(self, logger: logging.Logger, client: HttpClient,
                             entity: FileCacheEntity, record: Record,
                             field_name: str, url: str) -> Optional[List[Path]]:
        """download and store or find existing local copy of the url, return path to the local file"""
        external_path = self._external_name(entity, field_name, record)
        if external_path is not None:
            files = find_file(external_path)
            if files:
                return files
        internal_path = self._internal_name(entity, field_name, record)
        files = find_file(internal_path)
        if files and not entity.no_reuse:
            return files
        if entity.no_reuse:
            internal_path = find_free_suffix(internal_path, ' [{i}]')
        try:
            internal_path.parent.mkdir(parents=True, exist_ok=True)
        except OSError as e:
            logger.warning(f'failed to create directory hierarchy "{internal_path.parent}", skipping record {record!r}')
            return None
        file = await self._download_file(logger, client, url, internal_path)
        if file is not None:
            return [file]
        return None

    async def _download_file(self, logger: logging.Logger, client: HttpClient, url: str, path: Path) -> Optional[Path]:
        """download file from given url, store to path, return path as string with file:// schema"""
        temp_path = path.with_suffix(self.conf.partial_file_suffix)
        info = await download(self.concurrency_limit, logger, client, url, temp_path)
        if info is None:
            return None
        final_path = path.with_suffix(info.extension)
        ok = move_file(temp_path, final_path, logger)
        if not ok:
            return None
        return final_path

    def _external_name(self, entity: FileCacheEntity, field_name: str, record: Record) -> Optional[Path]:
        path = entity.resources_paths.get(field_name)
        name = entity.resources_files.get(field_name)
        if path is None or name is None:
            return None
        file = self._build_path(entity.resources_directory, path, name, record)
        return file

    def _internal_name(self, entity: FileCacheEntity, field_name: str, record: Record) -> Path:
        path = f'{field_name}/'
        name = f'{record.get_uid()}'
        file = self._build_path(self.conf.cache_directory, path, name, record)
        return file

    def _build_path(self, base_dir: Optional[Path], path_template: str, name_template: str, record: Record) -> Path:
        path = Fmt.format_path(path_template, record)
        if base_dir is not None and not path.is_absolute():
            path = base_dir / path
        name = Fmt.format(name_template, record, sanitize=True)
        full_name = path / name
        return full_name
